Classes_train = Classes_train[rowSums(Curves_train)>min_num_tweets]
Curves_train = Curves_train[rowSums(Curves_train)>min_num_tweets, ]
Curves_test = Curves_test[ , start_time_test:D_test ]
static_test = static_test[rowSums(Curves_test)>min_num_tweets, ]
Classes_test = Classes_test[rowSums(Curves_test)>min_num_tweets]
Curves_test = Curves_test[rowSums(Curves_test)>min_num_tweets, ]
Curves_train = as.matrix(Curves_train)
set.seed(1234)
dim(Curves_train)
dim(Curves_test)
sample_train = sample(1:(dim(Curves_train)[1]), 300)
Curves_train = Curves_train[sample_train, ]
static_train = static_train[sample_train, ]
Classes_train = Classes_train[sample_train]
Curves_test = as.matrix(Curves_test)
gsFPCA.model = gsFPCA(X_dat_s = Curves_train,
Ys_train = Classes_train, static_covariates = NA,
pve = 0.95, k = NA, J = 14)
Curves_train
library(gFPCAClassif)
gsFPCA.model = gsFPCA(X_dat_s = Curves_train,
Ys_train = Classes_train, static_covariates = NA,
pve = 0.95, k = NA, J = 14)
Classes_train
table(Classes_trian)
table(Classes_train)
Curves_train = rbind(Curves_gtrain, Curves_btrain)
static_train = rbind.data.frame(genuine_static_train, bots_static_train)
Curves_test = rbind(Curves_gtest, Curves_btest)
static_test = rbind.data.frame(genuine_static_test, bots_static_test)
#Two Levels Levels Can easily change to 4 levels by changing the levels
Classes_train = c(rep(1, N_gtrain), rep(2, N_btrain))
Classes_train = as.factor(Classes_train)
Classes_test = c(rep(1, N_genuine-N_gtrain), rep(2, N_bots-N_btrain))
Classes_test = as.factor(Classes_test)
table(Classes_train)
min_num_tweets
#Two Levels Levels Can easily change to 4 levels by changing the levels
Classes_train = c(rep(1, N_gtrain), rep(2, N_btrain))
Classes_train = as.factor(Classes_train)
Classes_test = c(rep(1, N_genuine-N_gtrain), rep(2, N_bots-N_btrain))
Classes_test = as.factor(Classes_test)
Curves_train = rbind(Curves_gtrain, Curves_btrain)
static_train = rbind.data.frame(genuine_static_train, bots_static_train)
Curves_test = rbind(Curves_gtest, Curves_btest)
static_test = rbind.data.frame(genuine_static_test, bots_static_test)
# static_train$followers_count = scale( static_train$followers_count,
#                                       center = mean(static_train$f ollowers_count, na.rm=T), scale = sd(static_train$followers_count, na.rm=T) )
# static_test$followers_count = scale( static_test$followers_count,
#                                      center = mean(static_train$followers_count, na.rm=T), scale = sd(static_train$followers_count, na.rm=T))
# static_train$friends_count = scale( static_train$friends_count,
#                                     center = mean(static_train$friends_count, na.rm=T), scale = sd(static_train$friends_count, na.rm=T))
# static_test$friends_count = scale( static_test$friends_count,
#                                    center = mean(static_train$friends_count, na.rm=T), scale = sd(static_train$friends_count, na.rm=T))
#D = dim(Curves_train)[2]
#Two Levels Levels Can easily change to 4 levels by changing the levels
Classes_train = c(rep(1, N_gtrain), rep(2, N_btrain))
Classes_train = as.factor(Classes_train)
Classes_test = c(rep(1, N_genuine-N_gtrain), rep(2, N_bots-N_btrain))
Classes_test = as.factor(Classes_test)
min_num_tweets = 15
Curves_train = Curves_train[ , start_time:D ]
static_train = static_train[rowSums(Curves_train)>min_num_tweets, ]
Classes_train = Classes_train[rowSums(Curves_train)>min_num_tweets]
Curves_train = Curves_train[rowSums(Curves_train)>min_num_tweets, ]
Curves_test = Curves_test[ , start_time_test:D_test ]
static_test = static_test[rowSums(Curves_test)>min_num_tweets, ]
Classes_test = Classes_test[rowSums(Curves_test)>min_num_tweets]
Curves_test = Curves_test[rowSums(Curves_test)>min_num_tweets, ]
dim(Curves_train)
Curves_train = as.matrix(Curves_train)
set.seed(1234)
sample_train = sample(1:(dim(Curves_train)[1]), 300)
Curves_train = Curves_train[sample_train, ]
static_train = static_train[sample_train, ]
Classes_train = Classes_train[sample_train]
Curves_test = as.matrix(Curves_test)
gsFPCA.model = gsFPCA(X_dat_s = Curves_train,
Ys_train = Classes_train, static_covariates = NA,
pve = 0.95, k = NA, J = 14)
X_dat_s = Curves_train
Ys_train = Classes_train
static_covariates = NA
pve = 0.95
k = NA
J = 14
bs0 = "cr"
D = dim(X_dat_s)[2]
N = dim(X_dat_s)[1]
tt=seq(0,1, len=D)
if(!is.na(static_covariates)[1]){
if((N != (dim(static_covariates)[1])) ){
stop("Dimensions of Covariates and Binary Curves do not match")
}
}
if(N != length(Ys_train)){
stop("Dimensions of Covariates and Binary Curves do not match")
}
##
#Step 1 of the proposed method
##
vec = matrix(1:(N), ncol = 1)
smoothed_x = logit(t(apply(vec, 1, function(x) regression_g(x, X_dat_s, tt, k=k, bs0 = bs0))))
vec
smoothed_x = logit(t(apply(vec, 1, function(x) regression_g(x, X_dat_s, tt, k=k, bs0 = bs0))))
regression_g
bs0
z=1
Curves = X_dat_s
tt
method="REML"
z1 = Curves[z,]
gam1 <- suppressWarnings(gam(z1~s(tt, bs = bs0, m=2, k = k),
family="binomial", method = method))
z1
c(z1)
bs0
gam1 <- suppressWarnings(gam(z1~s(tt, bs = "cr", m=2, k = k),
family="binomial", method = method))
gam1 <- suppressWarnings(gam(z1~s(tt, bs = "cr", m=2, k = k),
family="binomial", method = "REML"))
z1
dim(z1)
length(z1)
length(tt)
gam(z1~s(tt))
gam(z1~s(tt), family = "binomial")
?gam
gam(z1~s(tt, bs = "cr", m=2, k = k),
family="binomial", method = "REML")
k
k=10
gam1 <- suppressWarnings(gam(z1~s(tt, bs = "cr", m=2, k = k),
family="binomial", method = "REML"))
k
gsFPCA.model = gsFPCA(X_dat_s = Curves_train,
Ys_train = Classes_train, static_covariates = NA,
pve = 0.95, J = 14)
matplot(gsFPCA.model$eigen_funcs, type="l")
plot(gsFPCA.model$mu_t)
dim(Curves_train)
k
regression_g
?s
k= -1
?ns
?gam
?gam
library(gFPCAClassif)
gsFPCA.model = gsFPCA(X_dat_s = Curves_train,
Ys_train = Classes_train, static_covariates = NA,
pve = 0.95, J = 14)
matplot(gsFPCA.model$eigen_funcs, type="l")
library(fdapace)
citation("fdapace")
install.packages("fdapace")
citation("fdapace")
library(gFPCAClassif)
curves_train
summary(Curves_train)
dim(Curves_train)
#saveRDS(Curves_test, file = "D:/Research/Staicu/R_package/Package/gFPCAClassif/data/Curves_test.RDS")
save(Curves_test, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/Curves_test.rda")
Curves_test
Curves_test = Curves_test
Curves_train = Curves_train
static_test = static_test
static_train = static_train
Curves_test = Curves_test
Curves_train = Curves_train
static_test = static_test
static_train = static_train
Classes_test = Classes_test
Classes_train = Classes_train
write.csv(static_train, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/static_train.csv")
write.csv(static_test, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/static_test.csv")
Classes_test
which(Classes_test==2)
which(Classes_train==2)
classes_train
Classes_train
static_train$group = Classes_train
write.csv(static_train, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/static_train.csv")
library(readr)
dataset <- read_csv(NULL)
View(dataset)
library(readr)
static_train <- read_csv("data/static_train.csv")
View(static_train)
library(readr)
static_test <- read_csv("data/static_test.csv")
View(static_test)
dim(Curves_train)
c(Curves_train[1,1:100])
names(Curves_train[1,1:100])
names(Curves_train)
type(Curves_train)
Curves_train = cbind(c(1:200), Curves_train)
Curves_test = cbind(c(200:486), Curves_test)
dim(v)
dim(Curves_test)
Curves_test = cbind(c(201:486), Curves_test)
counter = 0
for(i in 1:5){
for(i2 in i:5)
counter = counter+1
}
counter
counter = 0
J = 100
for(i in 1:J){
for(i2 in i:J){
counter = counter+1
}
}
counter
J*(J-1)
(J-1)*(J-2)
i2
counter = 0
J = 100
for(i in 1:J){
if(i!=J){
for(i2 in i:J){
counter = counter+1
}
}
}
counter
counter = 0
J = 100
for(i in 1:J){
if(i!=J){
for(i2 in (i+1):J){
counter = counter+1
}
}
}
counter
(J-1)*(J-2)
counter = 0
J = 100
for(i in 1:(J-1)){
if(i!=J){
for(i2 in (i+1):J){
counter = counter+1
}
}
}
counter
(J-1)*(J-2)
counter/(J-1)
counter/((J-1)*(J/2))
counter = 0
J = 100
for(i in 1:(J-1)){
if(i!=J){
for(i2 in (i+1):J){
counter = counter+1
}
}
}
counter
((J-1)*(J/2))
counter = 0
J = 5
for(i in 1:(J-1)){
if(i!=J){
for(i2 in (i+1):J){
counter = counter+1
}
}
}
counter
((J-1)*(J/2))
?fpca.face?fpca.facefpca.face
?fpca.face
static_train <- read_csv("data/static_train.csv")
static_test <- read_csv("data/static_test.csv")
Curves_test
dim(Curves_test)
Curves_train = cbind(c(1:200), Curves_train)
Curves_test = cbind(c(201:486), Curves_test)
Curves_train[[1]]
Curves_train[1]
#saveRDS(Curves_test, file = "D:/Research/Staicu/R_package/Package/gFPCAClassif/data/Curves_test.RDS")
save(Curves_test, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/Curves_test.rda")
save(Curves_train, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/Curves_train.rda")
save(static_test, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/static_test.rda")
save(static_train, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/static_train.rda")
save(static_test, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/covariates_test.rda")
save(static_train, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/covariates_train.rda")
Curves_train
head(Covariates_train)
head(covariates_train)
covariates_train
covariates_test
library(gFPCAClassif)
covariates_test
static_test
covariates_train = static_train
covariates_test = static_test
save(covariates_test, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/covariates_test.rda")
save(covariates_train, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/covariates_train.rda")
library(gFPCAClassif)
library(gFPCAClassif)
covariates_train
gmFPCA_predict
library(gFPCAClassif)
Curves_train
dim(Curves_train)
gsFPCA.model = gsFPCA(X_dat_s = Curves_train,
Ys_train = Classes_train, static_covariates = NA,
pve = 0.95, k = 10, J = 14)
gsFPCA.model = gsFPCA(X_dat_s = Curves_train,
Ys_train = Classes_train, static_covariates = NA,
pve = 0.95, Kb = 10, J = 14)
gsFPCA
gsFPCA.model = gsFPCA(X_dat_s = Curves_train,
Ys = Classes_train, static_covariates = NA,
pve = 0.95, Kb = 10, num_knots = 14, bs0="cr")
gsFPCA.model = gsFPCA(X_dat_s = Curves_train,
Ys = Classes_train, covariates = NA,
pve = 0.95, Kb = 10, num_knots = 14, bs0="cr")
dim(Curves_test)
head(covariates_train)
matplot(gsFPCA.model$eigen_funcs)
static_train_cur = static_train[ , 2:4]
J
dim(Curves_train)
X_dat_m = t(matrix(t(Curves_train), nrow = 48))
dim(X_dat_m)
X_dat_m_test = t(matrix(t(Curves_test), nrow = 48))
dim(X_dat_m_test)
gmfpca.cur = gMFPCA(X_dat_m, Ys_train, Js, N, static_covariates = NA, gAR = T, pve1 = 0.95,
pve2 = 0.75, k = NA, q =3, approximation = "linear")
gmfpca.cur = gMFPCA(X_dat_m, Ys, Js, N, covariates = NA, gAR = T, pve1 = 0.95,
pve2 = 0.75, k = NA, q =3, approximation = "linear")
gmfpca.cur = gMFPCA(X_dat_m, Ys, Js, N, covariates = NA, gAR = T, pve1 = 0.95,
pve2 = 0.75, q =3, approximation = "linear")
gmfpca.cur = gMFPCA(X_dat_m, Ys=Ys_train, Js, N, covariates = NA, gAR = T, pve1 = 0.95,
pve2 = 0.75, q =3, approximation = "linear")
Ys_train
gmfpca.cur = gMFPCA(X_dat_m, Ys=Classes_train, Js, N, covariates = NA, gAR = T, pve1 = 0.95,
pve2 = 0.75, q =3, approximation = "linear")
gmfpca.cur = gMFPCA(X_dat_m, Ys=Classes_train, J, N, covariates = NA, gAR = T, pve1 = 0.95,
pve2 = 0.75, q =3, approximation = "linear")
gMFPCA
N
N = 200
gmfpca.cur = gMFPCA(X_dat_m, Ys=Classes_train, J, N, covariates = NA, gAR = T, pve1 = 0.95,
pve2 = 0.75, q =3, approximation = "linear")
gmfpca.cur$gar_models_ls[[1]]
gmfpca.cur = gMFPCA(X_dat_m, Ys=Classes_train, J, N, covariates = NA, gAR = T, pve1 = 0.95,
pve2 = 0.75, q =3, approximation = "linear")
N
dim(X_dat_m)
length(Ys)
Ys = Classes_train
Ys
2800/J
static_covariates = covariates
covariates
covariates = covariates_train
Ys_train = Ys
static_covariates = covariates
k = Kb
Kb= 5
k = Kb
D = dim(X_dat_m)[2]
if(length(Js)>1){
N = length(Js)
#set check if Js and N do not match up
}else{
N = dim(X_dat_m)[2]/Js
Js = rep(Js, N)
}
tt=seq(0,1, len=D)
Js = 14
if(length(Js)>1){
N = length(Js)
#set check if Js and N do not match up
}else{
N = dim(X_dat_m)[2]/Js
Js = rep(Js, N)
}
tt=seq(0,1, len=D)
if(!is.na(static_covariates)[1]){
if((N != (dim(static_covariates)[1])) ){
stop("Dimensions of Covariates and Binary Curves do not match")
}
}
!is.na(static_covariates)[1]
covariates
covariates=NA
static_covariates = covariates
k = Kb
D = dim(X_dat_m)[2]
if(length(Js)>1){
N = length(Js)
#set check if Js and N do not match up
}else{
N = dim(X_dat_m)[2]/Js
Js = rep(Js, N)
}
tt=seq(0,1, len=D)
if(!is.na(static_covariates)[1]){
if((N != (dim(static_covariates)[1])) ){
stop("Dimensions of Covariates and Binary Curves do not match")
}
}
if(N != length(Ys_train)){
stop("Dimensions of Covariates and Binary Curves do not match")
}
length(Ys_train)
N
(length(Js)
)
if(length(Js)>1){
N = length(Js)
#set check if Js and N do not match up
}else{
N = dim(X_dat_m)[2]/Js
Js = rep(Js, N)
}
N
dim(X_dat_m)[2]
dim(X_dat_m)[1]/Js
Js
Js = 14
if(length(Js)>1){
N = length(Js)
#set check if Js and N do not match up
}else{
N = dim(X_dat_m)[1]/Js
Js = rep(Js, N)
}
Js
N
gMFPCA <- function(X_dat_m, Ys, Js, N, covariates = NA, gAR = F, pve1 = 0.95,
pve2 = 0.95, Kb = 5, q = 3, approximation = "linear", gar_covariates = NA, bs0 = "cr"){
Ys_train = Ys
static_covariates = covariates
k = Kb
D = dim(X_dat_m)[2]
if(length(Js)>1){
N = length(Js)
#set check if Js and N do not match up
}else{
N = dim(X_dat_m)[1]/Js
Js = rep(Js, N)
}
tt=seq(0,1, len=D)
if(!is.na(static_covariates)[1]){
if((N != (dim(static_covariates)[1])) ){
stop("Dimensions of Covariates and Binary Curves do not match")
}
}
if(N != length(Ys_train)){
stop("Dimensions of Covariates and Binary Curves do not match")
}
J = Js[1]
posting_days = 1-(rowSums(X_dat_m)==0)
s_mat_train = t(matrix(as.numeric(matrix(posting_days, nrow = J)), nrow = J))
Js_s_train = rowSums(s_mat_train)
#Get the parsimonious distribution
if(approximation == "linear"){
cur.train = multilevel_linear_fpca(X_dat_m, J,
pve1 = pve1, pve2 = pve2, k = k, bs0 = bs0)
}else{
cur.train = multilevel_exponential_fpca(X_dat_m, J,
pve1 = pve1, pve2 = pve2, k = k, bs0 = bs0)
}
mu_t_hat = cur.train$mu_hat
eigen_vals1 = cur.train$eigen_vals1
eigen_funcs1 = cur.train$eigen_funcs1
eigen_vals2 = cur.train$eigen_vals2
eigen_funcs2 = cur.train$eigen_funcs2
posting_days = (rowSums(X_dat_m)>1)
s_mat_hat_train = t(matrix(as.numeric(matrix(posting_days, nrow = J)), nrow = J))
scores_train = estimate_scores(X_dat_m, s_mat = s_mat_train, I=N,  J=J,
eigen_vals1, eigen_vals2,
eigen_funcs1, eigen_funcs2, mu_t_hat)
return_vals = list( )
return_vals$scores_train = scores_train
return_vals$eigen_funcs1 = eigen_funcs1
return_vals$eigen_vals1 = eigen_vals1
return_vals$eigen_funcs2 = eigen_funcs2
return_vals$eigen_vals2 = eigen_vals2
return_vals$static_covariates = static_covariates
return_vals$classes = Ys_train
return_vals$mu_t = mu_t_hat
return_vals$gAR = gAR
return_vals$J = J
if(gAR){
gar_models_ls = list()
ng = length(unique(Ys_train))
for(l in 1:ng){
gar_models_ls[[l]] = fit_ajs_model(l, q, s_mat_hat_train, classes = Ys_train, static_train = gar_covariates)
}
return_vals$gar_models_ls = gar_models_ls
return_vals$s_mat_train = s_mat_train
return_vals$q = q
return_vals$gar_covariates = gar_covariates
}
return(gsFPCA.model = return_vals)
}
gmfpca.cur = gMFPCA(X_dat_m, Ys=Classes_train, J, N, covariates = NA, gAR = T, pve1 = 0.95,
pve2 = 0.75, q =3, approximation = "linear")
gmfpca.cur$eigen_funcs2
gmfpca.cur$eigen_funcs1
matplot(gmfpca.cur$eigen_funcs1)
