# static_test$friends_count = scale( static_test$friends_count,
#                                    center = mean(static_train$friends_count, na.rm=T), scale = sd(static_train$friends_count, na.rm=T))
#D = dim(Curves_train)[2]
#Two Levels Levels Can easily change to 4 levels by changing the levels
Classes_train = c(rep(1, N_gtrain), rep(2, N_btrain))
Classes_train = as.factor(Classes_train)
Classes_test = c(rep(1, N_genuine-N_gtrain), rep(2, N_bots-N_btrain))
Classes_test = as.factor(Classes_test)
Curves_train = Curves_train[ , start_time:D ]
static_train = static_train[rowSums(Curves_train)>min_num_tweets, ]
Classes_train = Classes_train[rowSums(Curves_train)>min_num_tweets]
Curves_train = Curves_train[rowSums(Curves_train)>min_num_tweets, ]
#print(table(Classes_train))
Curves_test = Curves_test[ , start_time_test:D_test ]
static_test = static_test[rowSums(Curves_test)>min_num_tweets, ]
Classes_test = Classes_test[rowSums(Curves_test)>min_num_tweets]
Curves_test = Curves_test[rowSums(Curves_test)>min_num_tweets, ]
Curves_train = as.matrix(Curves_train)
set.seed(1234)
sample_train = sample(1:(dim(Curves_train)[1]), 200)
Curves_train = Curves_train[sample_train, ]
static_train = static_train[sample_train, ]
Classes_train = Classes_train[sample_train]
Curves_test = as.matrix(Curves_test)
Classes_test
keeps = c("Curves_train", "static_train", "Classes_train", "Curves_test", "static_test", "Curves_test", "J", "Classes_test")
rm(list=setdiff(ls(), keeps))
save.image("D:/Research/Staicu/R_package/Package/gFPCAClassif/data/social_media.RData")
library(gFPCAClassif)
load(socia_media)
load(social_media)
load("social_media")
load("D:/Research/Staicu/R_package/Package/gFPCAClassif/data/social_media.RData")
library(gFPCAClassif)
load("social_media")
load(social_media)
social_media
gfpcaClassif::social_media
gsFPCA
gsFPCA_predict
social_media
social_media.Rdata
load(social_media.Rdata)
library(gFPCAClassif)
social_media
social_media
load(social_media.Rdata)
load("D:/Research/Staicu/R_package/Package/gFPCAClassif/data/social_media.RData")
saveRDS(file = "data.RDS")
?saveRDS
saveRDS(Curves_test, file = "D:/Research/Staicu/R_package/Package/gFPCAClassif/data/Curves_test.RDS")
?save
save(file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/Curves_test/social_media_data.rda")
save(file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/social_media_data.rda")
save(Curves_test, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/Curves_test.rda")
library(gFPCAClassif)
Curves_test
load("D:/Research/Staicu/R_package/Package/gFPCAClassif/data/social_media.RData")
#saveRDS(Curves_test, file = "D:/Research/Staicu/R_package/Package/gFPCAClassif/data/Curves_test.RDS")
save(Curves_test, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/Curves_test.rda")
save(Curves_train, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/Curves_train.rda")
save(static_test, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/static_test.rda")
save(static_train, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/static_train.rda")
save(Classes_test, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/Classes_test.rda")
save(Classes_train, file="D:/Research/Staicu/R_package/Package/gFPCAClassif/data/Classes_train.rda")
library(gFPCAClassif)
Curves_test
Curves_train
static_test
static_train
Classes_test
Classes_train
table(classes_train)
table(Classes_train)
social_media_data
data(social_media_data)
cur_data = data(social_media_data)
cur_data
cur_data = data("social_media_data")
library(gFPCAClassif)
library(readr)
library(ggplot2)
library(parallel)
format_needed = T
tweets_FS_users <- read_csv("D:/Research/data/tweets_FS_users_2017_08_01_to_2017_12_01.csv")
Start_time= as.POSIXct("2014-08-01 00:00:01")
End_time = as.POSIXct("2014-12-01 00:00:01")
users=as.vector(unique(tweets_FS_users$userid))
genuine_tweets <- read_csv("D:/Research/2018_LAS/data/cresci-2017.csv/datasets_full.csv/genuine_accounts.csv/genuine_accounts.csv/tweets.csv")
library(readr)
genuine_users <- read_csv("D:/Research/2018_LAS/data/cresci-2017.csv/datasets_full.csv/genuine_accounts.csv/genuine_accounts.csv/users.csv")
genuine_tweets <- read_csv("D:/Research/2018_LAS/data/cresci-2017.csv/datasets_full.csv/genuine_accounts.csv/genuine_accounts.csv/tweets.csv")
genuine_tweets <- read_csv("D:/Research/2018_LAS/data/cresci-2017.csv/datasets_full.csv/genuine_accounts.csv/genuine_accounts.csv/tweets.csv")
genuine_tweets <- read_csv("D:/Research/2018_LAS/data/cresci-2017.csv/datasets_full.csv/genuine_accounts.csv/genuine_accounts.csv/genuine_tweets.csv")
#remove users that do not include time zones
genuine_users=subset(genuine_users, !is.na(genuine_users$time_zone))
#remove tweets of the users who do not include time zones
genuine_tweets = subset(genuine_tweets, genuine_tweets$user_id%in%unique(genuine_users$id))
bot_tweets <- read_csv("D:/Research/2018_LAS/data/cresci-2017.csv/datasets_full.csv/social_spambots_1.csv/social_spambots_1.csv/tweets.csv")
bot_users <- read_csv("D:/Research/2018_LAS/data/cresci-2017.csv/datasets_full.csv/social_spambots_1.csv/social_spambots_1.csv/users.csv")
bot_tweets <- read_csv("D:/Research/2018_LAS/data/cresci-2017.csv/datasets_full.csv/genuine_accounts.csv/genuine_accounts.csv/bots_tweets.csv")
hist(genuine_tweets$timestamp, breaks="months")
hist(bot_tweets$timestamp, breaks="months")
start_date="2014-09-01 00:00:01 UTC"
end_date="2014-10-01 00:00:01 UTC"
genuine_tweets = subset(genuine_tweets, genuine_tweets$timestamp>=start_date &
genuine_tweets$timestamp <= end_date)
bot_tweets = subset(bot_tweets, bot_tweets$timestamp>=start_date &
bot_tweets$timestamp <= end_date)
hist(genuine_tweets$timestamp, breaks="days")
hist(bot_tweets$timestamp, breaks="days")
summary(bot_tweets$retweeted_status_id)
bot_tweets$reweet = ifelse(bot_tweets$retweeted_status_id==0,0,1)
View(bot_tweets)
which(bot_tweets$reweet==1)[1:10]
bot_tweets$text[52]
bot_tweets$text[282]
genuine_tweets$reweet = ifelse(genuine_tweets$retweeted_status_id==0,0,1)
genuine_users$id[1]
gen_users_vec = unique(genuine_users$id)
bot_users_vec = unqiue(bot_users$id)
bot_users_vec = unique(bot_users$id)
per_retweet = by(bot_tweets$reweet, bot_tweets$id, mean))
per_retweet = by(bot_tweets$reweet, bot_tweets$id, mean)
bot_per_retweet = by(bot_tweets$reweet, bot_tweets$user_id, mean)
genuine_per_retweet = by(genuine_tweets$reweet, genuine_tweets$user_id, mean)
bot_per_retweet
bot_per_retweet = c(by(bot_tweets$reweet, bot_tweets$user_id, mean))
genuine_per_retweet = c(by(genuine_tweets$reweet, genuine_tweets$user_id, mean))
bot_per_retweet
mean(bot_per_retweet)
mean(genuine_per_retweet)
hist(bot_per_retweet)
hist(genuine_per_retweet)
bot_tweets$reweet
which(bot_tweets$retweeted)[1:10]
which(bot_tweets$retweet)[1:10]
bot_tweets$retweet = ifelse(bot_tweets$retweeted_status_id==0,0,1)
genuine_tweets$retweet = ifelse(genuine_tweets$retweeted_status_id==0,0,1)
which(bot_tweets$retweet)[1:10]
which(bot_tweets$retweet==1)[1:10]
bot_tweets$text[52]
bot_tweets$text[282]
bot_tweets$text[1]
bot_tweets$text[2]
#tweets_FS_users <- read_csv("D:/Research/data/tweets_FS_users_2017_08_01_to_2017_12_01.csv")
tweets_FS_users <- read_csv("D:/Research/data/tweets_FS_users_test1.csv")
remove(tweets_FS_users)
tweets <- read_csv("D:/Research/data/Twitter_release_2019/venezuela_201901_1_tweets_csv_hashed_1.csv",
col_types = cols(user_display_name = col_skip(),
user_reported_location = col_skip(),
user_profile_description = col_skip(),
user_profile_url = col_skip(), account_language = col_skip(),
tweet_language = col_skip(), tweet_text = col_skip(),
tweet_client_name = col_skip(),
in_reply_to_tweetid = col_skip(),
in_reply_to_userid = col_skip(),
quoted_tweet_tweetid = col_skip(),
is_retweet = col_skip(), retweet_userid = col_skip(),
retweet_tweetid = col_skip(), latitude = col_skip(),
longitude = col_skip(), quote_count = col_skip(),
reply_count = col_skip(), like_count = col_skip(),
retweet_count = col_skip(), hashtags = col_skip(),
urls = col_skip(), user_mentions = col_skip(),
poll_choices = col_skip()))
hist(tweets$tweet_time, breaks = "week")
Start_time = "2014-08-01 00:00:01 UTC"
End_time = "2014-12-01 00:00:01 UTC"
tweets = subset(tweets, tweets$tweet_time >= Start_time &
tweets$tweet_time < End_time &
!is.na(tweets$userid))
hist(tweets$tweet_time, breaks = "day")
length(unique(tweets$userid))
tweets <- read_csv("D:/Research/data/Twitter_release_2019/venezuela_201901_1_tweets_csv_hashed_1.csv",
col_types = cols(user_display_name = col_skip(),
user_reported_location = col_skip(),
user_profile_description = col_skip(),
user_profile_url = col_skip(), account_language = col_skip(),
tweet_language = col_skip(), tweet_text = col_skip(),
tweet_client_name = col_skip(),
in_reply_to_tweetid = col_skip(),
in_reply_to_userid = col_skip(),
quoted_tweet_tweetid = col_skip(),
#is_retweet = col_skip(), retweet_userid = col_skip(),
retweet_tweetid = col_skip(), latitude = col_skip(),
longitude = col_skip(), quote_count = col_skip(),
reply_count = col_skip(), like_count = col_skip(),
retweet_count = col_skip(), hashtags = col_skip(),
urls = col_skip(), user_mentions = col_skip(),
poll_choices = col_skip()))
hist(tweets$tweet_time, breaks = "week")
Start_time = "2014-08-01 00:00:01 UTC"
End_time = "2014-12-01 00:00:01 UTC"
tweets = subset(tweets, tweets$tweet_time >= Start_time &
tweets$tweet_time < End_time &
!is.na(tweets$userid))
hist(tweets$tweet_time, breaks = "day")
length(unique(tweets$userid))
FS_per_retweet = c(by(tweets$is_retweet, tweets$userid, mean))
FS_per_retweet
hist(bot_per_retweet)
hist(genuine_per_retweet)
hist(FS_per_retweet)
?hist
hist(bot_per_retweet, density = T)
hist(bot_per_retweet, probability = T)
hist(genuine_per_retweet,  probability = T)
hist(bot_per_retweet, freq = F)
hist(bot_per_retweet, freq = F)
hist(genuine_per_retweet, freq = F)
hist(FS_per_retweet, freq = F)
#### to build package
#### ctrl +  shift + B
library(readr)
timelines_genuine <- read_csv("D:/Research/Staicu/Multi_level_everything/Data/timelines_genuine2.csv")
timelines_bots <- read_csv("D:/Research/Staicu/Multi_level_everything/Data/timelines_bots2.csv")
genuine_static <- read_csv("D:/Research/Staicu/Covariates/Data/Genuine_static_covariates.csv")
bots_static <- read_csv("D:/Research/Staicu/Covariates/Data/bots_static_covariates.csv")
FS_timelines <- read_csv("D:/Research/data/timelines_FS_users_test1.csv")
FS_labels <- read_csv("D:/Research/data/labels_FS_users_test1.csv")
genuine_static = subset(genuine_static , genuine_static$id%in%timelines_genuine$X1)
bots_static = subset(bots_static , bots_static$id%in%timelines_bots$X1)
genuine_static <- genuine_static[order(genuine_static$id),]
timelines_genuine <- timelines_genuine[order(timelines_genuine$X1),]
bots_static <- bots_static[order(bots_static$id),]
timelines_bots <- timelines_bots[order(timelines_bots$X1),]
#Number of days to analyze
#J_for_analysis = 21
J_train = 14
J_test = 14
#number of minutes per break (multiple of 5)
num_mins = 30
#random_days
#variable to select random starting day (used in days sensitivity analysis)
Random_start_day = T
Random_end_day  = T
#Number of cores able to run code on
numCores_to_run = 16
#Get Number for iterations
ITER = 50
#Format all data
# this is not the number of days wanted to analyze
J = J_train
#Set D to the end of the J days
# number of days * number of 5 minute long intervals in a day
D = 50*24*2+1
#Format data for each group
timelines_genuine2 = timelines_genuine[,2:D]
timelines_bots2 = timelines_bots[,2:D]
timelines_fs12 = FS_timelines[,2:D]
#format to matrix
timelines_genuine2 = data.matrix(timelines_genuine2)
timelines_bots2 = data.matrix(timelines_bots2)
timelines_fs12 = data.matrix(timelines_fs12)
#make sparse for easier storage
timelines_genuine2 = Matrix(timelines_genuine2, sparse=T)
timelines_bots2 = Matrix(timelines_bots2, sparse=T)
timelines_fs1 = Matrix(timelines_fs12, sparse=T)
#names(genuine_static)
keep_list = c("id","followers_count", "friends_count",
"lang", "created_at", "description")
genuine_static = genuine_static[, names(genuine_static) %in%
keep_list]
bots_static = bots_static[, names(bots_static) %in%
keep_list]
min_num_tweets = 1
FS_labels2  = FS_labels$x[rowSums(timelines_fs1)>min_num_tweets]
timelines_fs1 = timelines_fs1[rowSums(timelines_fs1)>min_num_tweets,]
FS_labels2 = ifelse(FS_labels2=="Russian", "Russia", FS_labels2)
classes_in_analyses = c("Iran", "Russia", "Saudi Arabia", "Venezuela", "Turkey")
classes_in_analyses = c("Turkey", "Saudi Arabia")
timelines_fs1 = timelines_fs1[FS_labels2%in%classes_in_analyses,]
FS_labels2 = FS_labels2[FS_labels2%in%classes_in_analyses]
get_dates = function(x){
cur_date = paste(substring(x, 5, 10),
substring(x, str_length(x)-4,
str_length(x)),
sep="")
return(as.Date(as.POSIXct(cur_date, format="%b %d %Y",tz=Sys.timezone())))
}
cur_dates = genuine_static$created_at
genuine_static$created_at = Reduce(c, lapply(cur_dates, get_dates))
cur_dates = bots_static$created_at
bots_static$created_at = Reduce(c, lapply(cur_dates, get_dates))
set.seed(1234)
#matrix to record accuracies
acc_mat = matrix(NA, nrow = 5,  ncol = 1)
f1_mat  = matrix(NA, nrow = 5,  ncol = 1)
if(i%%10 == 0 || i==1){
print(paste("On Iteration ", i , " out of ", ITER))
}
#only select the first 14 days
J = J_train
#J times number of observations in a day
D = J*(60*24)/num_mins
D_test = J_test*(60*24)/num_mins
nd_train = 50 - J - 1
nd_test = 50 - J_test - 1
if(Random_start_day){
start_time = (sample((1:nd_train),1)-1)*48+1
start_time_test = start_time
}else{
start_time = 1
start_time_test = 1
}
D  = start_time + D - 1
if(Random_end_day){
start_time_test = (sample((1:nd_test),1)-1)*48+1
}
D_test  = start_time_test + D_test - 1
#start_time = 1
timelines_genuine2 = timelines_genuine2
timelines_bots2 = timelines_bots2
timelines_fs1_2 = timelines_fs1
#only include accounts which were active during those J days
# can remove if you want all accounts but run into issues with GLM and GAM.
min_num_tweets = 1
genuine_static = genuine_static[rowSums(timelines_genuine2)>min_num_tweets,]
bots_static = bots_static[rowSums(timelines_bots2)>min_num_tweets,]
timelines_genuine2 = timelines_genuine2[rowSums(timelines_genuine2)>min_num_tweets,]
timelines_bots2 = timelines_bots2[rowSums(timelines_bots2)>min_num_tweets,]
timelines_fs2_2 = timelines_fs1_2[rowSums(timelines_fs1_2)>min_num_tweets,]
#select training and testing sets for genuine accounts
N_genuine = dim(timelines_genuine2)[1]
p_train = 0.8
N_gtrain = round(N_genuine * p_train)
train_genuine = sample(1:N_genuine, N_gtrain)
Curves_gtrain = timelines_genuine2[train_genuine, ]
Curves_gtest = timelines_genuine2[-train_genuine, ]
genuine_static_train = genuine_static[train_genuine, ]
genuine_static_test = genuine_static[-train_genuine, ]
#select training and testing sets for bots
N_bots = dim(timelines_bots2)[1]
N_btrain = round(N_bots * p_train)
train_bots = sample(1:N_bots, N_btrain)
Curves_btrain = timelines_bots2[train_bots, ]
Curves_btest = timelines_bots2[-train_bots, ]
bots_static_train = bots_static[train_bots, ]
bots_static_test = bots_static[-train_bots, ]
#select training and testing sets for fs2
N_r = dim(timelines_fs2_2)[1]
N_rtrain = round(N_r * p_train)
train_r = sample(1:N_r, N_rtrain)
Curves_FStrain = timelines_fs2_2[train_r, ]
Curves_FStest = timelines_fs2_2[-train_r, ]
#FS_static_train = russian_static[train_r, ]
#FS_static_test = russian_static[-train_r, ]
FS_static_train = russian_static[train_r, ]
Curves_train = rbind(Curves_gtrain, Curves_btrain)
static_train = rbind.data.frame(genuine_static_train, bots_static_train)
Curves_test = rbind(Curves_gtest, Curves_btest)
static_test = rbind.data.frame(genuine_static_test, bots_static_test)
#Two Levels Levels Can easily change to 4 levels by changing the levels
Classes_train = c(rep(1, N_gtrain), rep(2, N_btrain))
Classes_train = as.factor(Classes_train)
Classes_test = c(rep(1, N_genuine-N_gtrain), rep(2, N_bots-N_btrain))
Classes_test = as.factor(Classes_test)
Curves_train = Curves_train[ , start_time:D ]
static_train = static_train[rowSums(Curves_train)>min_num_tweets, ]
Classes_train = Classes_train[rowSums(Curves_train)>min_num_tweets]
Curves_train = Curves_train[rowSums(Curves_train)>min_num_tweets, ]
Curves_test = Curves_test[ , start_time_test:D_test ]
static_test = static_test[rowSums(Curves_test)>min_num_tweets, ]
Classes_test = Classes_test[rowSums(Curves_test)>min_num_tweets]
Curves_test = Curves_test[rowSums(Curves_test)>min_num_tweets, ]
Curves_train = as.matrix(Curves_train)
set.seed(1234)
dim(Curves_train)
dim(Curves_test)
sample_train = sample(1:(dim(Curves_train)[1]), 300)
Curves_train = Curves_train[sample_train, ]
static_train = static_train[sample_train, ]
Classes_train = Classes_train[sample_train]
Curves_test = as.matrix(Curves_test)
gsFPCA.model = gsFPCA(X_dat_s = Curves_train,
Ys_train = Classes_train, static_covariates = NA,
pve = 0.95, k = NA, J = 14)
Curves_train
library(gFPCAClassif)
gsFPCA.model = gsFPCA(X_dat_s = Curves_train,
Ys_train = Classes_train, static_covariates = NA,
pve = 0.95, k = NA, J = 14)
Classes_train
table(Classes_trian)
table(Classes_train)
Curves_train = rbind(Curves_gtrain, Curves_btrain)
static_train = rbind.data.frame(genuine_static_train, bots_static_train)
Curves_test = rbind(Curves_gtest, Curves_btest)
static_test = rbind.data.frame(genuine_static_test, bots_static_test)
#Two Levels Levels Can easily change to 4 levels by changing the levels
Classes_train = c(rep(1, N_gtrain), rep(2, N_btrain))
Classes_train = as.factor(Classes_train)
Classes_test = c(rep(1, N_genuine-N_gtrain), rep(2, N_bots-N_btrain))
Classes_test = as.factor(Classes_test)
table(Classes_train)
min_num_tweets
#Two Levels Levels Can easily change to 4 levels by changing the levels
Classes_train = c(rep(1, N_gtrain), rep(2, N_btrain))
Classes_train = as.factor(Classes_train)
Classes_test = c(rep(1, N_genuine-N_gtrain), rep(2, N_bots-N_btrain))
Classes_test = as.factor(Classes_test)
Curves_train = rbind(Curves_gtrain, Curves_btrain)
static_train = rbind.data.frame(genuine_static_train, bots_static_train)
Curves_test = rbind(Curves_gtest, Curves_btest)
static_test = rbind.data.frame(genuine_static_test, bots_static_test)
# static_train$followers_count = scale( static_train$followers_count,
#                                       center = mean(static_train$f ollowers_count, na.rm=T), scale = sd(static_train$followers_count, na.rm=T) )
# static_test$followers_count = scale( static_test$followers_count,
#                                      center = mean(static_train$followers_count, na.rm=T), scale = sd(static_train$followers_count, na.rm=T))
# static_train$friends_count = scale( static_train$friends_count,
#                                     center = mean(static_train$friends_count, na.rm=T), scale = sd(static_train$friends_count, na.rm=T))
# static_test$friends_count = scale( static_test$friends_count,
#                                    center = mean(static_train$friends_count, na.rm=T), scale = sd(static_train$friends_count, na.rm=T))
#D = dim(Curves_train)[2]
#Two Levels Levels Can easily change to 4 levels by changing the levels
Classes_train = c(rep(1, N_gtrain), rep(2, N_btrain))
Classes_train = as.factor(Classes_train)
Classes_test = c(rep(1, N_genuine-N_gtrain), rep(2, N_bots-N_btrain))
Classes_test = as.factor(Classes_test)
min_num_tweets = 15
Curves_train = Curves_train[ , start_time:D ]
static_train = static_train[rowSums(Curves_train)>min_num_tweets, ]
Classes_train = Classes_train[rowSums(Curves_train)>min_num_tweets]
Curves_train = Curves_train[rowSums(Curves_train)>min_num_tweets, ]
Curves_test = Curves_test[ , start_time_test:D_test ]
static_test = static_test[rowSums(Curves_test)>min_num_tweets, ]
Classes_test = Classes_test[rowSums(Curves_test)>min_num_tweets]
Curves_test = Curves_test[rowSums(Curves_test)>min_num_tweets, ]
dim(Curves_train)
Curves_train = as.matrix(Curves_train)
set.seed(1234)
sample_train = sample(1:(dim(Curves_train)[1]), 300)
Curves_train = Curves_train[sample_train, ]
static_train = static_train[sample_train, ]
Classes_train = Classes_train[sample_train]
Curves_test = as.matrix(Curves_test)
gsFPCA.model = gsFPCA(X_dat_s = Curves_train,
Ys_train = Classes_train, static_covariates = NA,
pve = 0.95, k = NA, J = 14)
X_dat_s = Curves_train
Ys_train = Classes_train
static_covariates = NA
pve = 0.95
k = NA
J = 14
bs0 = "cr"
D = dim(X_dat_s)[2]
N = dim(X_dat_s)[1]
tt=seq(0,1, len=D)
if(!is.na(static_covariates)[1]){
if((N != (dim(static_covariates)[1])) ){
stop("Dimensions of Covariates and Binary Curves do not match")
}
}
if(N != length(Ys_train)){
stop("Dimensions of Covariates and Binary Curves do not match")
}
##
#Step 1 of the proposed method
##
vec = matrix(1:(N), ncol = 1)
smoothed_x = logit(t(apply(vec, 1, function(x) regression_g(x, X_dat_s, tt, k=k, bs0 = bs0))))
vec
smoothed_x = logit(t(apply(vec, 1, function(x) regression_g(x, X_dat_s, tt, k=k, bs0 = bs0))))
regression_g
bs0
z=1
Curves = X_dat_s
tt
method="REML"
z1 = Curves[z,]
gam1 <- suppressWarnings(gam(z1~s(tt, bs = bs0, m=2, k = k),
family="binomial", method = method))
z1
c(z1)
bs0
gam1 <- suppressWarnings(gam(z1~s(tt, bs = "cr", m=2, k = k),
family="binomial", method = method))
gam1 <- suppressWarnings(gam(z1~s(tt, bs = "cr", m=2, k = k),
family="binomial", method = "REML"))
z1
dim(z1)
length(z1)
length(tt)
gam(z1~s(tt))
gam(z1~s(tt), family = "binomial")
?gam
gam(z1~s(tt, bs = "cr", m=2, k = k),
family="binomial", method = "REML")
k
k=10
gam1 <- suppressWarnings(gam(z1~s(tt, bs = "cr", m=2, k = k),
family="binomial", method = "REML"))
k
gsFPCA.model = gsFPCA(X_dat_s = Curves_train,
Ys_train = Classes_train, static_covariates = NA,
pve = 0.95, J = 14)
matplot(gsFPCA.model$eigen_funcs, type="l")
plot(gsFPCA.model$mu_t)
dim(Curves_train)
k
regression_g
?s
k= -1
?ns
?gam
?gam
library(gFPCAClassif)
gsFPCA.model = gsFPCA(X_dat_s = Curves_train,
Ys_train = Classes_train, static_covariates = NA,
pve = 0.95, J = 14)
matplot(gsFPCA.model$eigen_funcs, type="l")
